# -*- coding: utf-8 -*-
"""DB1_script_experiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NSjG_fyFHL-eG4w-ozOdCT1gQs0dl2Vd

#### import
"""

from google.colab import drive
drive.mount('/content/drive')

"""'/content/drive/MyDrive/EMG_Dataset/DB1_preprocessed_data'"""

!pip install  keras_preprocessing

!pip install keras-tcn

"""
Temporal Convolutional Network (TCN) implemented in Keras
This implementation is based on the original paper of Bai Shaojie, Kolter J Zico and Koltun Vladlen.
# References
- [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](http://arxiv.org/abs/1803.01271)
"""
import os, sys
          #F:\PhD\EMG_All\Project_Data_paper\NinaProDB\Musa_NinaPro_Project\All_Code
# os.chdir('F:/PhD/EMG_All/Project_Data_paper/NinaProDB/Musa_NinaPro_Project/All_Code/')
sys.path.append('/content/drive/MyDrive/EMG_Dataset/')
import preprocessing
from keras import backend as K
from keras.models import Model
from keras.layers import Input, Activation, Add, SpatialDropout1D, Masking, BatchNormalization
from keras import regularizers, initializers
from custom_layers import CausalConv1D
import numpy as np


def TCN(input_shape=None, residual_blocks=3, tcn_layers=-1, filters=12, filters_size=3,
        dropout_rate=None, weight_decay=1e-4, depth=40, seed=0, masking=False, mask_value=-10.0):
    """
        Creating a Temporal Convolutional Network. Supports masking.

        Arguments:
            input_shape     : array-like, shape of the input (height, width, depth)
            residual_blocks : amount of residual blocks that will be created (default: 3)
            tcn_layers      : number of layers in each residual block. You can also use a list for numbers of layers [2,4,3]
                              or define only 2 to add 2 layers at all residual blocks. -1 means that dense_layers will be calculated
                              by the given depth (default: -1)
            filters         : filters per layer
            filters_size    : filter size per layer
            dropout_rate    : defines the dropout rate that is accomplished after each conv layer (except the first one).
            weight_decay    : weight decay of L2 regularization on weights (default: 1e-4)
            depth           : number or layers (default: 40)
            seed            : rng seed
            masking         : whether to use masking. If True, mask value is -10.0

        Returns:
            Model        : A Keras model instance
    """

    if type(tcn_layers) is list:
        if len(tcn_layers) != residual_blocks:
            raise AssertionError(
                'Number of residual blocks have to be same length to specified layers')
    elif tcn_layers == -1:
        tcn_layers = depth // residual_blocks
        tcn_layers = [tcn_layers for _ in range(residual_blocks)]
    else:
        tcn_layers = [tcn_layers for _ in range(residual_blocks)]

    if type(filters) is list:
        if type(tcn_layers) is list:
            for i, layer_size in enumerate(tcn_layers):
                if layer_size != len(filters[i]):
                    raise AssertionError(
                        'Number of filters have to be same to layers. Found filters {}, layers {}'.format(np.sum(np.array(filters).shape), np.sum(tcn_layers)))
        elif np.prod(np.array(filters).shape) != np.sum(tcn_layers):
            raise AssertionError(
                'Number of filters have to be same to layers. Found filters {}, layers {}'.format(np.sum(np.array(filters).shape), np.sum(tcn_layers)))
    else:
        filters = [[filters for _ in range(tcn_layers[i])] for i in range(residual_blocks)]

    if type(filters_size) is list:
        if np.prod(np.array(filters_size).shape) != np.sum(tcn_layers):
            raise AssertionError(
                'Number of filters_size have to be same to layers. Found filters_size {}, layers {}'.format(np.sum(np.array(filters_size).shape), np.sum(tcn_layers)))
    else:
        filters_size = [[filters_size for _ in range(tcn_layers[i])] for i in range(residual_blocks)]

    seq_input = Input(shape=input_shape)
    rf = 1
    nb_dilation = 1

    print('Creating TCN')
    print('#############################################')
    print('Residual blocks: %s' % residual_blocks)
    print('Layers per residual block: %s' % tcn_layers)
    print('Filters per layer: %s' % filters)
    print('Filters size per layer: %s' % filters_size)

    kernel_init = initializers.glorot_normal(seed=seed)
    kernel_regl = regularizers.l2(weight_decay)

    x = seq_input
    if masking:
        x = Masking(mask_value)(seq_input)

    # Building residual blocks
    for block in range(residual_blocks):

        # Add residual block
        x, nb_dilation, rf = residual_block(x, tcn_layers[block], filters[block], filters_size[block], nb_dilation, rf, dropout_rate, kernel_regl, kernel_init)

    print('Last layer receptive field: %s' % rf)
    print('#############################################')

    return Model(seq_input, x, name='tcn')


def residual_block(x, nb_layers, nb_channels, filters, nb_dilation, rf, dropout_rate=None, regularizer=None, initializer='glorot_uniform'):
    """
    Creates a residual block and concatenates inputs
    """
    cb = CausalConv1D(nb_channels[-1], (1,),
                use_bias=True,
                kernel_regularizer=regularizer, kernel_initializer=initializer)(x)
    for i in range(nb_layers):
        x, nb_dilation, rf = tcn_block(
            x, nb_channels[i], filters[i], nb_dilation, rf, dropout_rate, regularizer, initializer)
    x = Add()([cb, x])
    return x, nb_dilation, rf


def tcn_block(x, nb_channels, filter_size, nb_dilation, rf, dropout_rate=None, regularizer=None, initializer='glorot_uniform'):
    """
    Creates a convolution block consisting of Conv-ReLU-WeightNorm-Dropout.
    Optional: dropout
    """

    # Standard (Conv-ReLU-WeightNorm-Dropout)
    x = CausalConv1D(nb_channels, filter_size,
                use_bias=True,
                dilation_rate=nb_dilation,
                kernel_regularizer=regularizer, kernel_initializer=initializer)(x)
    x = Activation('relu')(x)

    # Dropout
    if dropout_rate:
        x = SpatialDropout1D(dropout_rate)(x)

    rf = rf + (filter_size-1)*nb_dilation
    nb_dilation *= 2
    return x, nb_dilation, rf

"""### model"""

import copy
# import torch

# import torch.nn as nn
from typing import Optional
from keras import backend as K, regularizers, constraints, initializers
from keras.layers import Conv1D, Layer
import numpy as np
import tensorflow as tf

def dot_product(x, kernel):
    """
    Wrapper for dot product operation, in order to be compatible with both
    Theano and Tensorflow
    Args:
        x (): input
        kernel (): weights
    Returns:
    """
    if K.backend() == 'tensorflow':
        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)
    else:
        return K.dot(x, kernel)


class MeanOverTime(Layer):
    """
    Layer that computes the mean of timesteps returned from an RNN and supports masking
    Example:
        activations = LSTM(64, return_sequences=True)(words)
        mean = MeanOverTime()(activations)
    """

    def __init__(self, **kwargs):
        self.supports_masking = True
        super(MeanOverTime, self).__init__(**kwargs)

    def call(self, x, mask=None):
        if mask is not None:
            mask = K.cast(mask, 'float32')
            return K.cast(K.sum(x, axis=1) / K.sum(mask, axis=1, keepdims=True),
                          K.floatx())
        else:
            return K.mean(x, axis=1)

    def compute_output_shape(self, input_shape):
        return input_shape[0], input_shape[-1]

    def compute_mask(self, input, input_mask=None):
        return None

class AttentionWithContext(Layer):
    """
        Attention operation, with a context/query vector, for temporal data.
        Supports Masking.
        Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]
        "Hierarchical Attention Networks for Document Classification"
        by using a context vector to assist the attention
        # Input shape
            3D tensor with shape: `(samples, steps, features)`.
        # Output shape
            2D tensor with shape: `(samples, features)`.
        :param kwargs:
        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.
        The dimensions are inferred based on the output shape of the RNN.
        Example:
            model.add(LSTM(64, return_sequences=True))
            model.add(AttentionWithContext())
    """

    def __init__(self,
                 W_regularizer=None, u_regularizer=None, b_regularizer=None,
                 W_constraint=None, u_constraint=None, b_constraint=None,
                 bias=True,
                 return_attention=False, **kwargs):

        self.supports_masking = True
        self.return_attention = return_attention
        self.init = initializers.get('glorot_uniform')

        self.W_regularizer = regularizers.get(W_regularizer)
        self.u_regularizer = regularizers.get(u_regularizer)
        self.b_regularizer = regularizers.get(b_regularizer)

        self.W_constraint = constraints.get(W_constraint)
        self.u_constraint = constraints.get(u_constraint)
        self.b_constraint = constraints.get(b_constraint)

        self.bias = bias
        super(AttentionWithContext, self).__init__(**kwargs)

    def build(self, input_shape):
        assert len(input_shape) == 3
        print(type(input_shape))
        self.W = self.add_weight(shape=tf.TensorShape((input_shape[-1], input_shape[-1],)).as_list(),
                                 initializer=self.init,
                                 name='{}_W'.format(self.name),
                                 regularizer=self.W_regularizer,
                                 constraint=self.W_constraint)
        if self.bias:
            self.b = self.add_weight(shape=tf.TensorShape((input_shape[-1], )).as_list(),
                                     initializer='zero',
                                     name='{}_b'.format(self.name),
                                     regularizer=self.b_regularizer,
                                     constraint=self.b_constraint)

        self.u = self.add_weight(shape=tf.TensorShape((input_shape[-1], )).as_list(),
                                 initializer=self.init,
                                 name='{}_u'.format(self.name),
                                 regularizer=self.u_regularizer,
                                 constraint=self.u_constraint)

        super(AttentionWithContext, self).build(input_shape)

    def compute_mask(self, input, input_mask=None):
        # do not pass the mask to the next layers
        return None

    def call(self, x, mask=None):
        uit = dot_product(x, self.W)

        if self.bias:
            uit += self.b

        uit = K.tanh(uit)
        ait = dot_product(uit, self.u)

        a = K.exp(ait)

        # apply mask after the exp. will be re-normalized next
        if mask is not None:
            # Cast the mask to floatX to avoid float64 upcasting in theano
            a *= K.cast(mask, K.floatx())

        # in some cases especially in the early stages of training the sum may be almost zero
        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.
        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())

        a = K.expand_dims(a)
        weighted_input = x * a
        result = K.sum(weighted_input, axis=1)

        if self.return_attention:
            return [result, a]
        return result

    def compute_output_shape(self, input_shape):
        if self.return_attention:
            return [(input_shape[0], input_shape[-1]),
                    (input_shape[0], input_shape[1])]
        else:
            return input_shape[0], input_shape[-1]

    def get_config(self):
        config = {
            'bias': self.bias,
            'return_attention': self.return_attention
        }
        base_config = super(AttentionWithContext, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


class CausalConv1D(Conv1D):
    """
        Layer that performs 1D temporal causal convolutions. Supports masking.
        # Input shape
            3D tensor with shape: `(batch, steps, channels)`
        # Output shape
            3D tensor with shape: `(batch, steps, filters)`
    """
    def __init__(self, filters,
                 kernel_size,
                 strides=1,
                 dilation_rate=1,
                 activation=None,
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 bias_initializer='zeros',
                 kernel_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 kernel_constraint=None,
                 bias_constraint=None,
                 **kwargs):
        super(CausalConv1D, self).__init__(
            filters=filters,
            kernel_size=kernel_size,
            strides=strides,
            padding='causal',
            #padding='valid',
            dilation_rate=dilation_rate,
            activation=activation,
            use_bias=use_bias,
            kernel_initializer=kernel_initializer,
            bias_initializer=bias_initializer,
            kernel_regularizer=kernel_regularizer,
            bias_regularizer=bias_regularizer,
            activity_regularizer=activity_regularizer,
            kernel_constraint=kernel_constraint,
            bias_constraint=bias_constraint,
            **kwargs)
        self.supports_masking = True

    def call(self, inputs, mask=None):
        if mask is not None:
            if K.ndim(mask) == K.ndim(inputs) - 1:
                mask = K.expand_dims(mask)

            inputs *= K.cast(mask, K.floatx())

        output = K.conv1d(
            inputs, self.kernel,
            strides=self.strides[0],
            padding=self.padding,
            data_format=self.data_format,
            dilation_rate=self.dilation_rate[0]
        )
        ''' tf.keras.backend.conv1d(
            x,
            kernel,
            strides=1,
            padding='valid',
            data_format=None,
            dilation_rate=1
        )
        '''

        if self.use_bias:
            m = K.not_equal(output, 0)
            output = K.bias_add(
                output,
                self.bias,
                data_format=self.data_format)
            output *= K.cast(m, K.floatx())

        # Apply activations on the image
        if self.activation is not None:
            output = self.activation(output)

        return output

    def compute_output_shape(self, input_shape):
        return (input_shape[0], input_shape[1], self.filters)

    def compute_mask(self, inputs, mask=None):
        if isinstance(mask, list):
            mask = mask[0]
        output_mask = mask
        return output_mask

    def get_config(self):
        config = {
            'supports_masking': self.supports_masking
        }
        base_config = super(CausalConv1D, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

def scaled_dot_product_attention(q, k, v, mask):
  """アテンションの重みの計算
  q, k, vは最初の次元が一致していること
  k, vは最後から2番めの次元が一致していること
  マスクは型（パディングかルックアヘッドか）によって異なるshapeを持つが、
  加算の際にブロードキャスト可能であること
  引数：
    q: query shape == (..., seq_len_q, depth)
    k: key shape == (..., seq_len_k, depth)
    v: value shape == (..., seq_len_v, depth_v)
    mask: (..., seq_len_q, seq_len_k) にブロードキャスト可能な
          shapeを持つ浮動小数点テンソル。既定値はNone

  戻り値：
    出力、アテンションの重み
  """

  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)

  # matmul_qkをスケール
  dk = tf.cast(tf.shape(k)[-1], tf.float32)
  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)

  # マスクをスケール済みテンソルに加算
  if mask is not None:
    scaled_attention_logits += (mask * -1e9)

  # softmax は最後の軸(seq_len_k)について
  # 合計が1となるように正規化
  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)

  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)

  return output, attention_weights

class MultiHeadAttention(tf.keras.layers.Layer):
  def __init__(self, d_model, num_heads):
    super(MultiHeadAttention, self).__init__()
    self.num_heads = num_heads
    self.d_model = d_model

    assert d_model % self.num_heads == 0

    self.depth = d_model // self.num_heads

    self.wq = tf.keras.layers.Dense(d_model)
    self.wk = tf.keras.layers.Dense(d_model)
    self.wv = tf.keras.layers.Dense(d_model)

    self.dense = tf.keras.layers.Dense(d_model)

  def split_heads(self, x, batch_size):
    """最後の次元を(num_heads, depth)に分割。
    結果をshapeが(batch_size, num_heads, seq_len, depth)となるようにリシェイプする。
    """
    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
    return tf.transpose(x, perm=[0, 2, 1, 3])

  def call(self, v, k, q, mask=None):
    batch_size = tf.shape(q)[0]

    q = self.wq(q)  # (batch_size, seq_len, d_model)
    k = self.wk(k)  # (batch_size, seq_len, d_model)
    v = self.wv(v)  # (batch_size, seq_len, d_model)

    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)
    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)
    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)

    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)
    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)
    scaled_attention, attention_weights = scaled_dot_product_attention(
        q, k, v, mask)

    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)

    concat_attention = tf.reshape(scaled_attention,
                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)

    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)

    return output, attention_weights

from keras import backend as K, regularizers, constraints, initializers
from tensorflow.keras.layers import Layer

class MeanOverTime(Layer):
    """
    Layer that computes the mean of timesteps returned from an RNN and supports masking
    Example:
        activations = LSTM(64, return_sequences=True)(words)
        mean = MeanOverTime()(activations)
    """

    def __init__(self, **kwargs):
        self.supports_masking = True
        super(MeanOverTime, self).__init__(**kwargs)

    def call(self, x, mask=None):
        if mask is not None:
            mask = K.cast(mask, 'float32')
            return K.cast(K.sum(x, axis=1) / K.sum(mask, axis=1, keepdims=True),
                          K.floatx())
        else:
            return K.mean(x, axis=1)

    def compute_output_shape(self, input_shape):
        return input_shape[0], input_shape[-1]

    def compute_mask(self, input, input_mask=None):
        return None

def scaled_dot_product_attention(q, k, v, mask):
  """アテンションの重みの計算
  q, k, vは最初の次元が一致していること
  k, vは最後から2番めの次元が一致していること
  マスクは型（パディングかルックアヘッドか）によって異なるshapeを持つが、
  加算の際にブロードキャスト可能であること
  引数：
    q: query shape == (..., seq_len_q, depth)
    k: key shape == (..., seq_len_k, depth)
    v: value shape == (..., seq_len_v, depth_v)
    mask: (..., seq_len_q, seq_len_k) にブロードキャスト可能な
          shapeを持つ浮動小数点テンソル。既定値はNone

  戻り値：
    出力、アテンションの重み
  """

  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)

  # matmul_qkをスケール
  dk = tf.cast(tf.shape(k)[-1], tf.float32)
  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)

  # マスクをスケール済みテンソルに加算
  if mask is not None:
    scaled_attention_logits += (mask * -1e9)

  # softmax は最後の軸(seq_len_k)について
  # 合計が1となるように正規化
  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)

  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)

  return output, attention_weights

class MultiHeadAttention(tf.keras.layers.Layer):
  def __init__(self, d_model, num_heads):
    super(MultiHeadAttention, self).__init__()
    self.num_heads = num_heads
    self.d_model = d_model

    assert d_model % self.num_heads == 0

    self.depth = d_model // self.num_heads

    self.wq = tf.keras.layers.Dense(d_model)
    self.wk = tf.keras.layers.Dense(d_model)
    self.wv = tf.keras.layers.Dense(d_model)

    self.dense = tf.keras.layers.Dense(d_model)

  def split_heads(self, x, batch_size):
    """最後の次元を(num_heads, depth)に分割。
    結果をshapeが(batch_size, num_heads, seq_len, depth)となるようにリシェイプする。
    """
    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
    return tf.transpose(x, perm=[0, 2, 1, 3])

  def call(self, v, k, q, mask=None):
    batch_size = tf.shape(q)[0]

    q = self.wq(q)  # (batch_size, seq_len, d_model)
    k = self.wk(k)  # (batch_size, seq_len, d_model)
    v = self.wv(v)  # (batch_size, seq_len, d_model)

    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)
    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)
    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)

    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)
    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)
    scaled_attention, attention_weights = scaled_dot_product_attention(
        q, k, v, mask)

    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)

    concat_attention = tf.reshape(scaled_attention,
                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)

    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)

    return output, attention_weights

"""### train"""

from keras.layers import *
from tcn import TCN
from keras.models import Model

input_shape = (None, 10)

input = Input(shape=input_shape)

m1 = LSTM(64, return_sequences=True)(input)
tcn1 = TCN(nb_filters=64, nb_stacks=1, kernel_size=3, return_sequences=True)(m1)

m2 = Conv1D(32, 3)(input)
tcn2 = TCN(nb_filters=64, nb_stacks=1, kernel_size=3, return_sequences=True)(m2)

tcn3 = TCN(nb_filters=64, nb_stacks=1, kernel_size=3, return_sequences=True)(input)

# Flatten the sequences before concatenating
tcn1_flat = GlobalAveragePooling1D()(tcn1)
tcn2_flat = GlobalAveragePooling1D()(tcn2)
tcn3_flat = GlobalAveragePooling1D()(tcn3)
input_flat = GlobalAveragePooling1D()(input)

x = concatenate([tcn1_flat, tcn2_flat, tcn3_flat, input_flat])
x = Dropout(0.2)(x)
# x = MeanOverTime()(x)
output = Dense(53, trainable=True, activation='softmax')(x)

model_ = Model(inputs=input, outputs=output)
# model_.summary()

from tensorflow.keras.utils import plot_model
plot_model(model_, show_shapes=True,)

import numpy as np
import tensorflow as tf
import random
import os, sys
          #F:\PhD\EMG_All\Project_Data_paper\NinaProDB\Musa_NinaPro_Project\All_Code
# os.chdir('F:/PhD/EMG_All/Project_Data_paper/NinaProDB/Musa_NinaPro_Project/All_Code/')
sys.path.append('/content/drive/MyDrive/EMG_Dataset/')
import preprocessing
from generator import DataGenerator
# from models_ import getNetwork
import json
import scipy
import keras
import tensorflow as tf
from keras import optimizers, initializers, regularizers, constraints
from utils import *
from sklearn import metrics

import tensorflow as tf
from tensorflow import keras

os.getcwd()
# os.chdir('F:/PhD/EMG_All/Project_Data_paper/NinaProDB/')
print(os.getcwd())
# The below is necessary in Python 3.2.3 onwards to
# have reproducible behavior for certain hash-based operations.
# See these references for further details:
# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED
# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926

folder_process_data = '/content/drive/MyDrive/EMG_Dataset/DB1_preprocessed_data'
import os
os.environ['PYTHONHASHSEED'] = '0'

# The below is necessary for starting Numpy generated random numbers
# in a well-defined initial state.
np.random.seed(1234)

# The below is necessary for starting core Python generated random numbers
# in a well-defined state.
random.seed(12345)

# Force TensorFlow to use single thread.
# Multiple threads are a potential source of
# non-reproducible results.
# For further details, see:
#    https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res

config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth=True
sess = tf.compat.v1.Session(config=config)

session_conf = tf.compat.v1.ConfigProto(
    intra_op_parallelism_threads=0,
    inter_op_parallelism_threads=0
)
session_conf.gpu_options.allow_growth = True
from keras import backend as K

# The below tf.set_random_seed() will make random number generation
# in the TensorFlow backend have a well-defined initial state.
# For further details, see:
#   https://www.tensorflow.org/api_docs/python/tf/set_random_seed
#tf.set_random_seed(1234)
tf.random.set_seed(1234)
#tf.compat.v1.Session()
sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)
# K.set_session(sess)

##############################################################################


# print('Keras:', keras.version.VERSION)
print('Tensorflow:', tf.version.VERSION)

# 1. Logging
'''
if len(sys.argv) == 4:
    CONFIG_FILE = str(sys.argv[1])
    SUBJECT = int(sys.argv[2])
    TIMESTAMP = int(sys.argv[3])
else:
    print('Expected different number of arguments. {} were given'.format(len(sys.argv) - 1))
    sys.exit()
    '''
CONFIG_FILE='/content/drive/MyDrive/EMG_Dataset/config/TCCNet_aot_300.json'
from datetime import datetime
# from datetime import date
#TIMESTAMP=date

SUBJECT = 4

GESTURES=53
currentDateAndTime = datetime.now()

current_date = datetime.now()
TIMESTAMP = current_date .strftime("%Y%m%d%H%M%S")

#print(TIMESTAMP)

#TIMESTAMP=($(date +"%Y-%m-%d %H:%M:%S %s"))


with open(CONFIG_FILE) as json_file:
    config_params = json.load(json_file)
outdir='/content/drive/MyDrive/EMG_Dataset/Result'
LOGGING_FILE_PREFIX = config_params['logging']['log_file'] + '_' + str(TIMESTAMP)
if config_params['logging']['enable']:
    LOGGING_FILE = '/content/drive/MyDrive/EMG_Dataset/Result/L_' + LOGGING_FILE_PREFIX + '.log'
    LOGGING_TENSORBOARD_FILE = '/content/drive/MyDrive/EMG_Dataset/Result/tblogs/L_' + LOGGING_FILE_PREFIX

if config_params['model']['save']:
    MODEL_SAVE_FILE = '/content/drive/MyDrive/EMG_Dataset/Result/models/O1_' + LOGGING_FILE_PREFIX + '_{}.json'
    MODEL_WEIGHTS_SAVE_FILE = '/content/drive/MyDrive/EMG_Dataset/Result/models/O2_' + LOGGING_FILE_PREFIX + '_{}.h5'

METRICS_SAVE_FILE = '/content/drive/MyDrive/EMG_Dataset/Result/metrics/O3_' + LOGGING_FILE_PREFIX + '_{}.mat'


if not os.path.exists(os.path.dirname(METRICS_SAVE_FILE)):
    try:
        os.makedirs(os.path.dirname(METRICS_SAVE_FILE))
    except OSError as exc: # Guard against race condition
        if exc.errno != errno.EEXIST:
            raise

if not os.path.exists(os.path.dirname(MODEL_SAVE_FILE)):
    try:
        os.makedirs(os.path.dirname(MODEL_SAVE_FILE))
    except OSError as exc: # Guard against race condition
        if exc.errno != errno.EEXIST:
            raise

if not os.path.exists(os.path.dirname(LOGGING_TENSORBOARD_FILE)):
    try:
        os.makedirs(os.path.dirname(LOGGING_TENSORBOARD_FILE))
    except OSError as exc: # Guard against race condition
        if exc.errno != errno.EEXIST:
            raise



print('Logging file: {}'.format(LOGGING_FILE))
print('Tensorboard file: {}'.format(LOGGING_TENSORBOARD_FILE))
print('Model JSON file: {}'.format(MODEL_SAVE_FILE))
print('Model H5 file: {}'.format(MODEL_WEIGHTS_SAVE_FILE))
print('Metrics file: {}'.format(METRICS_SAVE_FILE))

# 2. Config params generator
PARAMS_TRAIN_GENERATOR = DEFAULT_GENERATOR_PARAMS.copy()
params_gen = config_params['dataset'].get('train_generator', {}).copy()
for key in params_gen.keys():
    PARAMS_TRAIN_GENERATOR[key] = params_gen[key]

PARAMS_VALID_GENERATOR = DEFAULT_GENERATOR_PARAMS.copy()
params_gen = config_params['dataset'].get('valid_generator', {}).copy()
for key in params_gen.keys():
    PARAMS_VALID_GENERATOR[key] = params_gen[key]

# 3. Initialization
#INPUT_DIRECTORY = '../dataset/Ninapro-DB1-Proc'
INPUT_DIRECTORY = folder_process_data        #link of process dataset
PARAMS_TRAIN_GENERATOR['preprocess_function_1'] = [preprocessing.lpf]
PARAMS_TRAIN_GENERATOR['preprocess_function_1_extra'] = [{'fs': 100}]
PARAMS_TRAIN_GENERATOR['data_type'] = 'rms'
PARAMS_TRAIN_GENERATOR['classes'] = [i for i in range(53)]

PARAMS_VALID_GENERATOR['preprocess_function_1'] = [preprocessing.lpf]
PARAMS_VALID_GENERATOR['preprocess_function_1_extra'] = [{'fs': 100}]
PARAMS_VALID_GENERATOR['data_type'] = 'rms'
PARAMS_VALID_GENERATOR['classes'] = [i for i in range(53)]

SUBJECTS = config_params['dataset'].get('subjects', [i for i in range(1, 28)])
if np.min(SUBJECTS) <= 0 or np.max(SUBJECTS) >= 28:
    raise AssertionError('Subject IDs should be between 1 and 27 inclusive for DB1. Were given {}\n'.format(SUBJECTS))

PARAMS_TRAIN_GENERATOR.pop('input_directory', '')
PARAMS_VALID_GENERATOR.pop('input_directory', '')

# MODEL = getNetwork(config_params['model']['name'])
# MODEL = getNetwork()

mean_train, mean_test, mean_test_3, mean_test_5 = [], [], [], []
mean_cm = []
mean_train_loss, mean_test_loss = [], []

if config_params['logging']['enable']:
    if os.path.isfile(LOGGING_FILE) == False:
        with open(LOGGING_FILE, 'w') as f:
            f.write(
                'TIMESTAMP: {}\n'
                # 'KERAS: {}\n'
                # 'TENSORFLOW: {}\n'
                'DATASET: {}\n'
                'TRAIN_GENERATOR: {}\n'
                'VALID_GENERATOR: {}\n'
                'MODEL: {}\n'
                'MODEL_PARAMS: {}\n'
                'TRAIN_PARAMS: {}\n'.format(
                    TIMESTAMP,
                    # keras.__version__,
                    # tf.version.VERSION,
                    config_params['dataset']['name'], PARAMS_TRAIN_GENERATOR,
                    PARAMS_VALID_GENERATOR,
                    config_params['model']['name'], config_params['model']['extra'],
                    config_params['training']
                )
            )
            f.write(
                'SUBJECT,TRAIN_SHAPE,TEST_SHAPE,TRAIN_LOSS,TRAIN_ACC,TEST_LOSS,TEST_ACC,TEST_TOP_3_ACC,TEST_TOP_5_ACC\n')

print('Subject: {}'.format(SUBJECT))
input_dir = '{}/subject-{:02d}'.format(INPUT_DIRECTORY, SUBJECT)
print(input_dir)

train_generator = DataGenerator(input_directory=input_dir, **PARAMS_TRAIN_GENERATOR)
valid_generator = DataGenerator(input_directory=input_dir, **PARAMS_VALID_GENERATOR)
X_test, Y_test, test_reps = valid_generator.get_data()                                  # X test,  Y test

# print(X_test)

# print('Train generator:')
# print(train_generator)
# print('Test generator:')
# print(valid_generator)

# model = MODEL

model = model_

# model.load_weights(path)

# model = MODEL(
#     input_shape=(None, 10),
#     classes=train_generator.n_classes,
#     **config_params['model']['extra'])
#model.summary()

if config_params['training']['optimizer'] == 'adam':
    optimizer = optimizers.Adam(lr=config_params['training']['l_rate'], epsilon=0.001)
elif config_params['training']['optimizer'] == 'sgd':
    optimizer = optimizers.SGD(lr=config_params['training']['l_rate'], momentum=0.9)

model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', top_3_accuracy, top_5_accuracy])

train_callbacks = []
'''
if config_params['logging']['enable']:
    tensorboardCallback = MyTensorboard(log_dir=LOGGING_TENSORBOARD_FILE + "/{}".format(SUBJECT),
                                        batch_size=100,
                                        histogram_freq=10)
    train_callbacks.append(tensorboardCallback)
'''
lrScheduler = MyLRScheduler(**config_params['training']['l_rate_schedule'])
train_callbacks.append(lrScheduler)

history = model.fit_generator(train_generator, epochs=150, #config_params['training']['epochs'],
                              validation_data=(X_test,Y_test), callbacks=train_callbacks, verbose=2)
Y_pred = model.predict(X_test)

y_pred = np.argmax(Y_pred, axis=1)
y_test = np.argmax(Y_test, axis=1)

if config_params['model']['save']:
    # serialize model to JSON
    model_json = model.to_json()
    with open(MODEL_SAVE_FILE.format(SUBJECT), "w") as json_file:
        json_file.write(model_json)
    # serialize weights to HDF5
    model.save_weights(MODEL_WEIGHTS_SAVE_FILE.format(SUBJECT))
    print("Saved model to disk")

# Confusion Matrix
# C_{i, j} is equal to the number of observations known to be in group i but predicted to be in group j.
cnf_matrix_frame = metrics.confusion_matrix(y_test, y_pred)
if np.array(mean_cm).shape != cnf_matrix_frame.shape:
    mean_cm = cnf_matrix_frame
else:
    mean_cm += cnf_matrix_frame

mean_train.append(history.history['accuracy'][-1])
mean_test.append(history.history['val_accuracy'][-1])
mean_train_loss.append(history.history['loss'][-1])
mean_test_loss.append(history.history['val_loss'][-1])
mean_test_3.append(history.history['val_top_3_accuracy'][-1])
mean_test_5.append(history.history['val_top_5_accuracy'][-1])

if config_params['logging']['enable']:
    with open(LOGGING_FILE, 'a') as f:
        f.write('{},{},{},{},{},{},{},{},{}\n'.format(SUBJECT, train_generator.__len__() * PARAMS_TRAIN_GENERATOR['batch_size'], valid_generator.__len__(),
            mean_train_loss[-1], mean_train[-1], mean_test_loss[-1], mean_test[-1], mean_test_3[-1], mean_test_5[-1]))

metrics_dict = {
    'mean_cm': mean_cm,
    'mean_test': mean_test,
    'mean_test_3': mean_test_3,
    'mean_test_5': mean_test_5,
    'mean_train': mean_train,
    'mean_train_loss': mean_train_loss,
    'mean_test_loss': mean_test_loss
}
scipy.io.savemat(METRICS_SAVE_FILE.format(SUBJECT), metrics_dict)