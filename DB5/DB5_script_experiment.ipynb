{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "941b9616-794a-4be5-b1a6-37f33ee50999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 14:43:21.797735: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-28 14:43:21.875215: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-28 14:43:22.245632: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-28 14:43:22.245666: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-28 14:43:22.245681: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "Keras: 2.11.0\n",
      "Tensorflow: 2.11.0\n",
      "Logging file: /home/sota/EMGcode/EMG_experiment/Result/L_TCCNet_att.log\n",
      "Tensorboard file: /home/sota/EMGcode/EMG_experiment/Result/tblogs/L_TCCNet_att\n",
      "Model JSON file: /home/sota/EMGcode/EMG_experiment/Result/models/O1_TCCNet_att_{}.json\n",
      "Model H5 file: /home/sota/EMGcode/EMG_experiment/Result/models/O2_TCCNet_att_{}.h5\n",
      "Metrics file: /home/sota/EMGcode/EMG_experiment/Result/metrics/O3_TCCNet_att_{}.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 14:43:23.189763: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-28 14:43:23.222934: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-28 14:43:23.224244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-28 14:43:23.225891: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-28 14:43:23.227177: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-28 14:43:23.228402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-28 14:43:23.229575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-28 14:43:23.312280: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-28 14:43:23.313238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-28 14:43:23.314129: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-28 14:43:23.315558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21914 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import sys, os\n",
    "sys.path.append('/home/sota/EMGcode/EMG_experiment/')\n",
    "import mypreprocessing\n",
    "from generator2 import DataGenerator\n",
    "from models import getNetwork\n",
    "from custom_layers import MeanOverTime\n",
    "import sys, os\n",
    "import json\n",
    "import scipy\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import optimizers, initializers, regularizers, constraints\n",
    "from utils import *\n",
    "from sklearn import metrics\n",
    "import matplotlib as plt\n",
    "from keras.layers import *\n",
    "from tcn import TCN\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pyp\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "folder_process_data = '/home/sota/EMGcode/EMG_experiment/DB5_pre/'\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "print('Keras:', keras.__version__)\n",
    "print('Tensorflow:', tf.__version__)\n",
    "\n",
    "# 1. Logging\n",
    "'''\n",
    "if len(sys.argv) == 4:\n",
    "    CONFIG_FILE = str(sys.argv[1])\n",
    "    SUBJECT = int(sys.argv[2])\n",
    "    TIMESTAMP = int(sys.argv[3])\n",
    "else:\n",
    "    print('Expected different number of arguments. {} were given'.format(len(sys.argv) - 1))\n",
    "    sys.exit()\n",
    "    '''\n",
    "CONFIG_FILE='/home/sota/EMGcode/EMG_experiment/config/TCCNet_aot_2 (1).json'\n",
    "\n",
    "SUBJECT=1\n",
    "GESTURES=52\n",
    "\n",
    "with open(CONFIG_FILE) as json_file:\n",
    "    config_params = json.load(json_file)\n",
    "outdir='/home/sota/EMGcode/EMG_experiment/DB9_pre/Result'\n",
    "LOGGING_FILE_PREFIX = config_params['logging']['log_file']\n",
    "if config_params['logging']['enable']:\n",
    "    LOGGING_FILE = '/home/sota/EMGcode/EMG_experiment/Result/L_' + LOGGING_FILE_PREFIX + '.log'# 'Musa_NinaPro_Project/ProcessedData/DB1/Result/L_' + LOGGING_FILE_PREFIX + '.log'\n",
    "    LOGGING_TENSORBOARD_FILE = '/home/sota/EMGcode/EMG_experiment/Result/tblogs/L_' + LOGGING_FILE_PREFIX #'Musa_NinaPro_Project/ProcessedData/DB1/Result/tblogs/L_' + LOGGING_FILE_PREFIX\n",
    "\n",
    "if config_params['model']['save']:\n",
    "    MODEL_SAVE_FILE = '/home/sota/EMGcode/EMG_experiment/Result/models/O1_' + LOGGING_FILE_PREFIX + '_{}.json'\n",
    "    MODEL_WEIGHTS_SAVE_FILE = '/home/sota/EMGcode/EMG_experiment/Result/models/O2_' + LOGGING_FILE_PREFIX + '_{}.h5'\n",
    "\n",
    "METRICS_SAVE_FILE = '/home/sota/EMGcode/EMG_experiment/Result/metrics/O3_' + LOGGING_FILE_PREFIX + '_{}.mat'\n",
    "\n",
    "if not os.path.exists(os.path.dirname(METRICS_SAVE_FILE)):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(METRICS_SAVE_FILE))\n",
    "    except OSError as exc: # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "if not os.path.exists(os.path.dirname(MODEL_SAVE_FILE)):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(MODEL_SAVE_FILE))\n",
    "    except OSError as exc: # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise\n",
    "            \n",
    "if not os.path.exists(os.path.dirname(LOGGING_TENSORBOARD_FILE)):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(LOGGING_TENSORBOARD_FILE))\n",
    "    except OSError as exc: # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "print('Logging file: {}'.format(LOGGING_FILE))\n",
    "print('Tensorboard file: {}'.format(LOGGING_TENSORBOARD_FILE))\n",
    "print('Model JSON file: {}'.format(MODEL_SAVE_FILE))\n",
    "print('Model H5 file: {}'.format(MODEL_WEIGHTS_SAVE_FILE))\n",
    "print('Metrics file: {}'.format(METRICS_SAVE_FILE))\n",
    "\n",
    "# 2. Config params generator\n",
    "PARAMS_TRAIN_GENERATOR = DEFAULT_GENERATOR_PARAMS.copy()\n",
    "params_gen = config_params['dataset'].get('train_generator', {}).copy()\n",
    "for key in params_gen.keys():\n",
    "    PARAMS_TRAIN_GENERATOR[key] = params_gen[key]\n",
    "\n",
    "PARAMS_VALID_GENERATOR = DEFAULT_GENERATOR_PARAMS.copy()\n",
    "params_gen = config_params['dataset'].get('valid_generator', {}).copy()\n",
    "for key in params_gen.keys():\n",
    "    PARAMS_VALID_GENERATOR[key] = params_gen[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c52a439-a7c9-48a9-8271-b4058f00b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K, regularizers, constraints, initializers\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"アテンションの重みの計算\n",
    "  q, k, vは最初の次元が一致していること\n",
    "  k, vは最後から2番めの次元が一致していること\n",
    "  マスクは型（パディングかルックアヘッドか）によって異なるshapeを持つが、\n",
    "  加算の際にブロードキャスト可能であること\n",
    "  引数：\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: (..., seq_len_q, seq_len_k) にブロードキャスト可能な\n",
    "          shapeを持つ浮動小数点テンソル。既定値はNone\n",
    "\n",
    "  戻り値：\n",
    "    出力、アテンションの重み\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  # matmul_qkをスケール\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # マスクをスケール済みテンソルに加算\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "  # softmax は最後の軸(seq_len_k)について\n",
    "  # 合計が1となるように正規化\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights\n",
    "\n",
    "class ChannelAttention(Layer):\n",
    "    def __init__(self, ratio = 8):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.channels = input_shape[-1]\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=self.channels // self.ratio,\n",
    "                                           activation='relu',\n",
    "                                           kernel_initializer='he_normal',\n",
    "                                           use_bias=True)\n",
    "        self.dense_output = tf.keras.layers.Dense(units=self.channels,\n",
    "                                                  kernel_initializer='he_normal',\n",
    "                                                  use_bias=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs = tf.expand_dims(inputs, axis=2)\n",
    "        gap = tf.reduce_mean(inputs, axis=[0, 1], keepdims=True)\n",
    "\n",
    "        attention = self.dense(gap)\n",
    "        attention = self.dense_output(attention)\n",
    "        attention = tf.keras.activations.sigmoid(attention)\n",
    "\n",
    "        output = tf.multiply(inputs, attention)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8bf343b-1aee-4fba-b761-5185e4c1929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_subject = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ddf331e-0b9e-4637-8491-0db1a3ba1380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 14:43:23.969212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-28 14:43:23.970299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-28 14:43:23.971242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-28 14:43:23.972202: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-28 14:43:23.973082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-28 14:43:23.973932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21914 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-11-28 14:43:23.975327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-28 14:43:23.976289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-28 14:43:23.977147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-28 14:43:23.978028: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-28 14:43:23.978860: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-28 14:43:23.979712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21914 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: 4\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "os.chdir('/home/sota/EMGcode/EMG_experiment/')\n",
    "import mypreprocessing\n",
    "from generator2 import DataGenerator\n",
    "#from models import getNetwork\n",
    "from DB2_Mydataset import *\n",
    "#from model.ninapro_network import *\n",
    "#from models import TCCNet\n",
    "from models import getNetwork\n",
    "import sys\n",
    "import json\n",
    "import scipy\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import optimizers, initializers, regularizers, constraints\n",
    "from utils import *\n",
    "from sklearn import metrics\n",
    "# print(DEFAULT_GENERATOR_PARAMS)\n",
    "os.getcwd()\n",
    "\n",
    "import argparse\n",
    "#import torch\n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# parser.add_argument(\"-b\", \"--batch_size\", type=int, default=128)  # 16\n",
    "# parser.add_argument(\"-lr\", \"--learning_rate\", type=float, default=1e-3)\n",
    "# parser.add_argument('--cuda', default=True, help='enables cuda')\n",
    "# parser.add_argument('-j', '--workers', default=8, type=int, metavar='N',\n",
    "#                     help='number of data loading workers (default: 8)')\n",
    "# parser.add_argument('--epochs', default=1000, type=int, metavar='N',\n",
    "#                     help='number of total epochs to run')  # 1000\n",
    "\n",
    "# parser.add_argument('--patiences', default=500, type=int,\n",
    "#                     help='number of epochs to tolerate no improvement of val_loss')  # 1000\n",
    "\n",
    "# '''\n",
    "# parser.add_argument('--test_subject_id', type=int, default=3,\n",
    "#                     help='id of test subject, for cross-validation')\n",
    "\n",
    "# parser.add_argument('--data_cfg', type=int, default=1,\n",
    "#                     help='0 for 14 class, 1 for 28')\n",
    "\n",
    "# '''\n",
    "# parser.add_argument('--dp_rate', type=float, default=0.1,\n",
    "#                     help='dropout rate')  # 1000\n",
    "\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n",
    "folder_process_data = '/home/sota/EMGcode/EMG_experiment/DB5_pre'\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "np.random.seed(1234)\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "random.seed(12345)\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see:\n",
    "#    https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "session_conf = tf.compat.v1.ConfigProto(\n",
    "    intra_op_parallelism_threads=0,\n",
    "    inter_op_parallelism_threads=0\n",
    ")\n",
    "session_conf.gpu_options.allow_growth = True\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "#   https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "tf.random.set_seed(1234)\n",
    "#tf.compat.v1.Session()\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# 1. Logging\n",
    "'''\n",
    "if len(sys.argv) == 4:\n",
    "    CONFIG_FILE = str(sys.argv[1])\n",
    "    SUBJECT = int(sys.argv[2])\n",
    "    TIMESTAMP = int(sys.argv[3])\n",
    "else:\n",
    "    print('Expected different number of arguments. {} were given'.format(len(sys.argv) - 1))\n",
    "    sys.exit()\n",
    "    '''\n",
    "CONFIG_FILE='/home/sota/EMGcode/EMG_experiment/config/TCCNet_aot_2 (1).json'\n",
    "\n",
    "SUBJECT=1\n",
    "GESTURES=52\n",
    "\n",
    "\n",
    "with open(CONFIG_FILE) as json_file:\n",
    "    config_params = json.load(json_file)\n",
    "outdir='/home/sota/EMGcode/EMG_experiment/DB9_pre/Result'\n",
    "LOGGING_FILE_PREFIX = config_params['logging']['log_file']\n",
    "if config_params['logging']['enable']:\n",
    "    LOGGING_FILE = '/home/sota/EMGcode/EMG_experiment/Result/L_' + LOGGING_FILE_PREFIX + '.log'# 'Musa_NinaPro_Project/ProcessedData/DB1/Result/L_' + LOGGING_FILE_PREFIX + '.log'\n",
    "    LOGGING_TENSORBOARD_FILE = '/home/sota/EMGcode/EMG_experiment/Result/tblogs/L_' + LOGGING_FILE_PREFIX #'Musa_NinaPro_Project/ProcessedData/DB1/Result/tblogs/L_' + LOGGING_FILE_PREFIX\n",
    "\n",
    "if config_params['model']['save']:\n",
    "    MODEL_SAVE_FILE = '/home/sota/EMGcode/EMG_experiment/Result/models/O1_' + LOGGING_FILE_PREFIX + '_{}.json'\n",
    "    MODEL_WEIGHTS_SAVE_FILE = '/home/sota/EMGcode/EMG_experiment/Result/models/O2_' + LOGGING_FILE_PREFIX + '_{}.h5'\n",
    "\n",
    "METRICS_SAVE_FILE = '/home/sota/EMGcode/EMG_experiment/Result/metrics/O3_' + LOGGING_FILE_PREFIX + '_{}.mat'\n",
    "\n",
    "\n",
    "if not os.path.exists(os.path.dirname(METRICS_SAVE_FILE)):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(METRICS_SAVE_FILE))\n",
    "    except OSError as exc: # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "if not os.path.exists(os.path.dirname(MODEL_SAVE_FILE)):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(MODEL_SAVE_FILE))\n",
    "    except OSError as exc: # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise\n",
    "            \n",
    "if not os.path.exists(os.path.dirname(LOGGING_TENSORBOARD_FILE)):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(LOGGING_TENSORBOARD_FILE))\n",
    "    except OSError as exc: # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise\n",
    "            \n",
    "# 2. Config params generator\n",
    "PARAMS_TRAIN_GENERATOR = DEFAULT_GENERATOR_PARAMS.copy()\n",
    "params_gen = config_params['dataset'].get('train_generator', {}).copy()\n",
    "for key in params_gen.keys():\n",
    "    PARAMS_TRAIN_GENERATOR[key] = params_gen[key]\n",
    "# print(PARAMS_TRAIN_GENERATOR)\n",
    "PARAMS_VALID_GENERATOR = DEFAULT_GENERATOR_PARAMS.copy()\n",
    "params_gen = config_params['dataset'].get('valid_generator', {}).copy()\n",
    "for key in params_gen.keys():\n",
    "    PARAMS_VALID_GENERATOR[key] = params_gen[key]\n",
    "\n",
    "# 3. Initialization\n",
    "#INPUT_DIRECTORY = '../dataset/Ninapro-DB1-Proc'\n",
    "INPUT_DIRECTORY = folder_process_data        #link of process dataset\n",
    "PARAMS_TRAIN_GENERATOR['preprocess_function_1'] = [mypreprocessing.lpf]\n",
    "PARAMS_TRAIN_GENERATOR['preprocess_function_1_extra'] = [{'fs': 100}]\n",
    "PARAMS_TRAIN_GENERATOR['data_type'] = 'rms'\n",
    "PARAMS_TRAIN_GENERATOR['classes'] = [i for i in range(1, 53)]\n",
    "PARAMS_TRAIN_GENERATOR['dim'] = [None,16]\n",
    "PARAMS_TRAIN_GENERATOR['repetitions'] = [1, 3, 4, 6]\n",
    "PARAMS_TRAIN_GENERATOR['size_factor'] = 10\n",
    "# PARAMS_TRAIN_GENERATOR['min_max_norm'] = True\n",
    "\n",
    "PARAMS_VALID_GENERATOR['preprocess_function_1'] = [mypreprocessing.lpf]\n",
    "PARAMS_VALID_GENERATOR['preprocess_function_1_extra'] = [{'fs': 100}]\n",
    "PARAMS_VALID_GENERATOR['data_type'] = 'rms'\n",
    "PARAMS_VALID_GENERATOR['classes'] = [i for i in range(1, 53)]\n",
    "PARAMS_VALID_GENERATOR['dim'] = [None,16]\n",
    "PARAMS_VALID_GENERATOR['repetitions'] = [2,5]\n",
    "PARAMS_VALID_GENERATOR['size_factor'] = 10\n",
    "# PARAMS_VALID_GENERATOR['min_max_norm'] = True\n",
    "\n",
    "# PARAMS_TRAIN_GENERATOR['batch_size'] = int(8)\n",
    "# PARAMS_VALID_GENERATOR['batch_size'] = int(1)\n",
    "PARAMS_TRAIN_GENERATOR['batch_size'] = int(128/4)\n",
    "PARAMS_VALID_GENERATOR['batch_size'] = int(128/4)\n",
    "learning_late = config_params['training']['l_rate']\n",
    "\n",
    "# PARAMS_TRAIN_GENERATOR['window_size'] = 200\n",
    "# PARAMS_VALID_GENERATOR['window_size'] = 200\n",
    "\n",
    "# PARAMS_TRAIN_GENERATOR['window_step'] = 100\n",
    "# PARAMS_VALID_GENERATOR['window_step'] = 100\n",
    "\n",
    "SUBJECTS = config_params['dataset'].get('subjects', [i for i in range(1, 11)])\n",
    "if np.min(SUBJECTS) <= 0 or np.max(SUBJECTS) >= 11:\n",
    "    raise AssertionError('Subject IDs should be between 1 and 27 inclusive for DB1. Were given {}\\n'.format(SUBJECTS))\n",
    "\n",
    "PARAMS_TRAIN_GENERATOR.pop('input_directory', '')\n",
    "PARAMS_VALID_GENERATOR.pop('input_directory', '')\n",
    "\n",
    "\n",
    "#MODEL = getNetwork(config_params['model']['name'])\n",
    "\n",
    "mean_train, mean_test, mean_test_3, mean_test_5 = [], [], [], []\n",
    "mean_cm = []\n",
    "mean_train_loss, mean_test_loss = [], []\n",
    "\n",
    "if config_params['logging']['enable']:\n",
    "    if os.path.isfile(LOGGING_FILE) == False:\n",
    "        with open(LOGGING_FILE, 'w') as f:\n",
    "            f.write(\n",
    "                'TIMESTAMP: {}\\n'\n",
    "                'KERAS: {}\\n'\n",
    "                'TENSORFLOW: {}\\n'\n",
    "                'DATASET: {}\\n'\n",
    "                'TRAIN_GENERATOR: {}\\n'\n",
    "                'VALID_GENERATOR: {}\\n'\n",
    "                'MODEL: {}\\n'\n",
    "                'MODEL_PARAMS: {}\\n'\n",
    "                'TRAIN_PARAMS: {}\\n'.format(\n",
    "                    keras.__version__, tf.__version__,\n",
    "                    config_params['dataset']['name'], PARAMS_TRAIN_GENERATOR,\n",
    "                    PARAMS_VALID_GENERATOR,\n",
    "                    config_params['model']['name'], config_params['model']['extra'],\n",
    "                    config_params['training']\n",
    "                )\n",
    "            )\n",
    "            f.write(\n",
    "                'SUBJECT,TRAIN_SHAPE,TEST_SHAPE,TRAIN_LOSS,TRAIN_ACC,TEST_LOSS,TEST_ACC,TEST_TOP_3_ACC,TEST_TOP_5_ACC\\n')\n",
    "\n",
    "print(f'Subject: {target_subject}')\n",
    "input_dir = '{}/subject-{:02d}'.format(INPUT_DIRECTORY, target_subject)\n",
    "# print(input_dir)\n",
    "\n",
    "train_generator = DataGenerator(input_directory=input_dir, **PARAMS_TRAIN_GENERATOR)\n",
    "valid_generator = DataGenerator(input_directory=input_dir, **PARAMS_VALID_GENERATOR)\n",
    "\n",
    "X_train, Y_train, train_reps,L_=train_generator.get_data()\n",
    "\n",
    "y_train=np.where(Y_train==1)[1]\n",
    "# print(y_train.shape)\n",
    "\n",
    "X_test, Y_test, test_reps = valid_generator.get_data()     \n",
    "                             # X test,  Y test\n",
    "# print(X_test.shape)  #shape   X=100*13083, 12  it is for 2 reptation  Y=100*50   \n",
    "# print(Y_test.shape)\n",
    "y_test=np.where(Y_test==1)[1]\n",
    "\n",
    "# print(y_test.shape)\n",
    "\n",
    "#reshape of x_train    reading*sample*channel*1\n",
    "X_train=X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[2],1)\n",
    "\n",
    "# print(X_train.shape[0])\n",
    "train_dataset = Hand_Dataset(X_train, y_train)   # for 1*8*22*3  label=1\n",
    "# print(train_dataset[1]['skeleton'].shape)\n",
    "    \n",
    "# print('Call Mydataset for testing :')\n",
    "X_test=X_test.reshape(X_test.shape[0],X_test.shape[1],X_test.shape[2],1)\n",
    "test_dataset =  Hand_Dataset(X_test, y_test)\n",
    "# print(test_dataset[1]['skeleton'].shape)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# args, unknown = parser.parse_known_args()\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=args.batch_size, shuffle=True,\n",
    "#     num_workers=args.workers, pin_memory=False)\n",
    "\n",
    "# val_loader = torch.utils.data.DataLoader(\n",
    "#     test_dataset,\n",
    "#     batch_size=args.batch_size, shuffle=False,\n",
    "#     num_workers=args.workers, pin_memory=False)\n",
    "\n",
    "class_num=train_generator.n_classes\n",
    "# print(class_num)\n",
    "\n",
    "timelen=X_train.shape[1]\n",
    "\n",
    "# print(timelen)\n",
    "\n",
    "# print('Train generator:')\n",
    "# print(train_generator)\n",
    "# print('Test generator:')\n",
    "# print(valid_generator)\n",
    "\n",
    "# print(train_generator.n_classes)\n",
    "# print(train_generator.repetitions)\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bcad753-bdbb-4891-9f88-860ed984a6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'repetitions': [1, 3, 4, 6], 'batch_size': 32, 'sample_weight': True, 'dim': [None, 16], 'classes': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52], 'shuffle': True, 'noise_snr_db': 25, 'scale_sigma': 0.0, 'window_size': 0, 'window_step': 0, 'rotation': 0, 'rotation_mask': None, 'time_warping': 0.2, 'mag_warping': 0.2, 'permutation': 0, 'data_type': 'rms', 'preprocess_function_1': [<function lpf at 0x7c0c543c8430>], 'preprocess_function_2': None, 'preprocess_function_1_extra': [{'fs': 100}], 'preprocess_function_2_extra': None, 'size_factor': 10, 'pad_len': None, 'pad_value': -10.0, 'min_max_norm': True, 'update_after_epoch': False, 'label_proc': None, 'label_proc_extra': None}\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "{'repetitions': [2, 5], 'batch_size': 32, 'sample_weight': False, 'dim': [None, 16], 'classes': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52], 'shuffle': False, 'noise_snr_db': 0, 'scale_sigma': 0.0, 'window_size': 0, 'window_step': 0, 'rotation': 0, 'rotation_mask': None, 'time_warping': 0.0, 'mag_warping': 0.0, 'permutation': 0, 'data_type': 'rms', 'preprocess_function_1': [<function lpf at 0x7c0c543c8430>], 'preprocess_function_2': None, 'preprocess_function_1_extra': [{'fs': 100}], 'preprocess_function_2_extra': None, 'size_factor': 10, 'pad_len': None, 'pad_value': -10.0, 'min_max_norm': True, 'update_after_epoch': False, 'label_proc': None, 'label_proc_extra': None}\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Classes: 52\n",
      "Class weights: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1.]\n",
      "Original dataset: 208\n",
      "Augmented dataset: 2288\n",
      "Number of sliding windows: 2288\n",
      "Batch size: 32\n",
      "Number of iterations: 71\n",
      "Window size: 0\n",
      "Window step: 0\n",
      "Pad length: 2779\n",
      "Output shape: (2779, 16)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Classes: 52\n",
      "Class weights: []\n",
      "Original dataset: 104\n",
      "Augmented dataset: 104\n",
      "Number of sliding windows: 104\n",
      "Batch size: 32\n",
      "Number of iterations: 3\n",
      "Window size: 0\n",
      "Window step: 0\n",
      "Pad length: 1063\n",
      "Output shape: (1063, 16)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(PARAMS_TRAIN_GENERATOR)\n",
    "print('---------------------------------------------------------------------------\\n')\n",
    "print(PARAMS_VALID_GENERATOR)\n",
    "print('---------------------------------------------------------------------------\\n')\n",
    "print(train_generator)\n",
    "print('---------------------------------------------------------------------------\\n')\n",
    "print(valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf58ec49-d27f-4b93-86f2-7584cad0669c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, None, 16)]   0           []                               \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, None, 2)      98          ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, None, 2)     12          ['conv1d_1[0][0]']               \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4 (Gl  (None, 2)           0           ['separable_conv1d_1[0][0]']     \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            3           ['global_average_pooling1d_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tcn_2 (TCN)                    (None, None, 64)     140096      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 2)            4           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, None, 64)    24832       ['tcn_2[0][0]']                  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)          (None, None, 2)      0           ['separable_conv1d_1[0][0]',     \n",
      "                                                                  'dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, None, 64)    72512       ['input_2[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_average_pooling1d_5 (Gl  (None, 64)          0           ['bidirectional_1[0][0]']        \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_6 (Gl  (None, 2)           0           ['multiply_1[0][0]']             \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_7 (Gl  (None, 64)          0           ['bidirectional_2[0][0]']        \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 130)          0           ['global_average_pooling1d_5[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'global_average_pooling1d_6[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'global_average_pooling1d_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 130)          0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " channel_attention_1 (ChannelAt  (None, 130)         2242        ['dropout_1[0][0]']              \n",
      " tention)                                                                                         \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 52)           6812        ['channel_attention_1[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 246,611\n",
      "Trainable params: 246,611\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from tcn import TCN\n",
    "from keras.models import Model\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "def se_block(in_block, ch, ratio=2):\n",
    "    z = GlobalAveragePooling1D()(in_block) \n",
    "    x = Dense(ch // ratio, activation='relu')(z)\n",
    "    x = Dense(ch, activation='sigmoid')(x)\n",
    "    return multiply([in_block, x])\n",
    "\n",
    "input_shape = (None, 16)\n",
    "\n",
    "input = Input(shape=input_shape)\n",
    "\n",
    "branch1 = TCN(nb_filters=32, nb_stacks=1, kernel_size=3, return_sequences=True)(input)\n",
    "branch1 = Bidirectional(LSTM(32, return_sequences=True))(branch1)\n",
    "\n",
    "branch2 = Conv1D(2, 3)(input)\n",
    "branch2 = SeparableConv1D(filters=2, kernel_size=3, padding='same', activation='relu')(branch2)\n",
    "branch2 = se_block(branch2, ch=2)\n",
    "\n",
    "branch3 = Bidirectional(TCN(nb_filters=32, nb_stacks=1, kernel_size=3, return_sequences=True))(input)\n",
    "\n",
    "# Flatten the sequences before concatenating\n",
    "branch1_flat = GlobalAveragePooling1D()(branch1)\n",
    "branch2_flat = GlobalAveragePooling1D()(branch2)\n",
    "branch3_flat = GlobalAveragePooling1D()(branch3)\n",
    "# input_flat = GlobalAveragePooling1D()(input)\n",
    "\n",
    "x = concatenate([branch1_flat, branch2_flat, branch3_flat])\n",
    "# x = concatenate([branch1_flat, branch2_flat, branch3_flat, input_flat])\n",
    "x = Dropout(0.2)(x)\n",
    "x = ChannelAttention()(x)\n",
    "output = Dense(52, trainable=True, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3266127-2601-4a9a-8e76-b84d13a4f208",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "3/3 [==============================] - 1s 22ms/step\n",
      "Epoch 1: saved predictions and true labels\n",
      "71/71 - 12s - loss: 11.1591 - accuracy: 0.0211 - top_3_accuracy: 0.0629 - top_5_accuracy: 0.1034 - val_loss: 3.9527 - val_accuracy: 0.0208 - val_top_3_accuracy: 0.0625 - val_top_5_accuracy: 0.0938 - 12s/epoch - 167ms/step\n",
      "Epoch 2/128\n",
      "3/3 [==============================] - 0s 18ms/step\n",
      "Epoch 2: saved predictions and true labels\n",
      "71/71 - 7s - loss: 3.9615 - accuracy: 0.0260 - top_3_accuracy: 0.0647 - top_5_accuracy: 0.0995 - val_loss: 3.9426 - val_accuracy: 0.0312 - val_top_3_accuracy: 0.0938 - val_top_5_accuracy: 0.1250 - 7s/epoch - 105ms/step\n",
      "Epoch 3/128\n",
      "3/3 [==============================] - 0s 18ms/step\n",
      "Epoch 3: saved predictions and true labels\n",
      "71/71 - 7s - loss: 3.8616 - accuracy: 0.0308 - top_3_accuracy: 0.0973 - top_5_accuracy: 0.1466 - val_loss: 3.7899 - val_accuracy: 0.0312 - val_top_3_accuracy: 0.1250 - val_top_5_accuracy: 0.1458 - 7s/epoch - 105ms/step\n",
      "Epoch 4/128\n",
      "3/3 [==============================] - 0s 17ms/step\n",
      "Epoch 4: saved predictions and true labels\n",
      "71/71 - 7s - loss: 3.6375 - accuracy: 0.0440 - top_3_accuracy: 0.1281 - top_5_accuracy: 0.2126 - val_loss: 3.5244 - val_accuracy: 0.0521 - val_top_3_accuracy: 0.1875 - val_top_5_accuracy: 0.2708 - 7s/epoch - 106ms/step\n",
      "Epoch 5/128\n",
      "3/3 [==============================] - 0s 29ms/step\n",
      "Epoch 5: saved predictions and true labels\n",
      "71/71 - 8s - loss: 3.3536 - accuracy: 0.0836 - top_3_accuracy: 0.2232 - top_5_accuracy: 0.3275 - val_loss: 2.9233 - val_accuracy: 0.1354 - val_top_3_accuracy: 0.4062 - val_top_5_accuracy: 0.5104 - 8s/epoch - 107ms/step\n",
      "Epoch 6/128\n",
      "3/3 [==============================] - 0s 18ms/step\n",
      "Epoch 6: saved predictions and true labels\n",
      "71/71 - 8s - loss: 2.9400 - accuracy: 0.1488 - top_3_accuracy: 0.3565 - top_5_accuracy: 0.4996 - val_loss: 3.0379 - val_accuracy: 0.1667 - val_top_3_accuracy: 0.3646 - val_top_5_accuracy: 0.5729 - 8s/epoch - 106ms/step\n",
      "Epoch 7/128\n",
      "3/3 [==============================] - 0s 20ms/step\n",
      "Epoch 7: saved predictions and true labels\n",
      "71/71 - 8s - loss: 2.8313 - accuracy: 0.1906 - top_3_accuracy: 0.4027 - top_5_accuracy: 0.5502 - val_loss: 2.2278 - val_accuracy: 0.2917 - val_top_3_accuracy: 0.6354 - val_top_5_accuracy: 0.8229 - 8s/epoch - 106ms/step\n",
      "Epoch 8/128\n",
      "3/3 [==============================] - 0s 18ms/step\n",
      "Epoch 8: saved predictions and true labels\n",
      "71/71 - 7s - loss: 2.4361 - accuracy: 0.2663 - top_3_accuracy: 0.5290 - top_5_accuracy: 0.6765 - val_loss: 1.8750 - val_accuracy: 0.3646 - val_top_3_accuracy: 0.6979 - val_top_5_accuracy: 0.8958 - 7s/epoch - 106ms/step\n",
      "Epoch 9/128\n",
      "3/3 [==============================] - 0s 21ms/step\n",
      "Epoch 9: saved predictions and true labels\n",
      "71/71 - 8s - loss: 2.1808 - accuracy: 0.3248 - top_3_accuracy: 0.6105 - top_5_accuracy: 0.7416 - val_loss: 1.9884 - val_accuracy: 0.3854 - val_top_3_accuracy: 0.7500 - val_top_5_accuracy: 0.9167 - 8s/epoch - 106ms/step\n",
      "Epoch 10/128\n",
      "3/3 [==============================] - 0s 18ms/step\n",
      "Epoch 10: saved predictions and true labels\n",
      "71/71 - 8s - loss: 2.1155 - accuracy: 0.3578 - top_3_accuracy: 0.6272 - top_5_accuracy: 0.7632 - val_loss: 1.8528 - val_accuracy: 0.3958 - val_top_3_accuracy: 0.7500 - val_top_5_accuracy: 0.9167 - 8s/epoch - 106ms/step\n",
      "Epoch 11/128\n",
      "3/3 [==============================] - 0s 18ms/step\n",
      "Epoch 11: saved predictions and true labels\n",
      "71/71 - 8s - loss: 1.8879 - accuracy: 0.4133 - top_3_accuracy: 0.7011 - top_5_accuracy: 0.8209 - val_loss: 1.8306 - val_accuracy: 0.5104 - val_top_3_accuracy: 0.7812 - val_top_5_accuracy: 0.9062 - 8s/epoch - 106ms/step\n",
      "Epoch 12/128\n",
      "3/3 [==============================] - 0s 19ms/step\n",
      "Epoch 12: saved predictions and true labels\n",
      "71/71 - 8s - loss: 1.7327 - accuracy: 0.4520 - top_3_accuracy: 0.7434 - top_5_accuracy: 0.8504 - val_loss: 1.4835 - val_accuracy: 0.5521 - val_top_3_accuracy: 0.8958 - val_top_5_accuracy: 0.9792 - 8s/epoch - 106ms/step\n",
      "Epoch 13/128\n",
      "3/3 [==============================] - 0s 18ms/step\n",
      "Epoch 13: saved predictions and true labels\n",
      "71/71 - 8s - loss: 1.5342 - accuracy: 0.5141 - top_3_accuracy: 0.7989 - top_5_accuracy: 0.8882 - val_loss: 1.1651 - val_accuracy: 0.6458 - val_top_3_accuracy: 0.9062 - val_top_5_accuracy: 0.9896 - 8s/epoch - 106ms/step\n",
      "Epoch 14/128\n",
      "3/3 [==============================] - 0s 19ms/step\n",
      "Epoch 14: saved predictions and true labels\n",
      "71/71 - 8s - loss: 1.5282 - accuracy: 0.5207 - top_3_accuracy: 0.8055 - top_5_accuracy: 0.8966 - val_loss: 1.3514 - val_accuracy: 0.5729 - val_top_3_accuracy: 0.8438 - val_top_5_accuracy: 0.9375 - 8s/epoch - 106ms/step\n",
      "Epoch 15/128\n",
      "3/3 [==============================] - 0s 18ms/step\n",
      "Epoch 15: saved predictions and true labels\n",
      "71/71 - 8s - loss: 1.3103 - accuracy: 0.5761 - top_3_accuracy: 0.8349 - top_5_accuracy: 0.9225 - val_loss: 1.3774 - val_accuracy: 0.6146 - val_top_3_accuracy: 0.8646 - val_top_5_accuracy: 0.9688 - 8s/epoch - 106ms/step\n",
      "Epoch 16/128\n",
      "3/3 [==============================] - 0s 20ms/step\n",
      "Epoch 16: saved predictions and true labels\n",
      "71/71 - 8s - loss: 1.2149 - accuracy: 0.5990 - top_3_accuracy: 0.8640 - top_5_accuracy: 0.9362 - val_loss: 0.9746 - val_accuracy: 0.7083 - val_top_3_accuracy: 0.9167 - val_top_5_accuracy: 1.0000 - 8s/epoch - 106ms/step\n",
      "Epoch 17/128\n",
      "3/3 [==============================] - 0s 18ms/step\n",
      "Epoch 17: saved predictions and true labels\n",
      "71/71 - 8s - loss: 1.2243 - accuracy: 0.6065 - top_3_accuracy: 0.8653 - top_5_accuracy: 0.9322 - val_loss: 0.9293 - val_accuracy: 0.6875 - val_top_3_accuracy: 0.9271 - val_top_5_accuracy: 1.0000 - 8s/epoch - 106ms/step\n",
      "Epoch 18/128\n",
      "3/3 [==============================] - 0s 17ms/step\n",
      "Epoch 18: saved predictions and true labels\n",
      "71/71 - 8s - loss: 1.0478 - accuracy: 0.6567 - top_3_accuracy: 0.8948 - top_5_accuracy: 0.9538 - val_loss: 1.2804 - val_accuracy: 0.6771 - val_top_3_accuracy: 0.9375 - val_top_5_accuracy: 0.9792 - 8s/epoch - 106ms/step\n",
      "Epoch 19/128\n",
      "3/3 [==============================] - 0s 19ms/step\n",
      "Epoch 19: saved predictions and true labels\n",
      "71/71 - 8s - loss: 1.0009 - accuracy: 0.6602 - top_3_accuracy: 0.9018 - top_5_accuracy: 0.9613 - val_loss: 1.3487 - val_accuracy: 0.6562 - val_top_3_accuracy: 0.9062 - val_top_5_accuracy: 1.0000 - 8s/epoch - 107ms/step\n",
      "Epoch 20/128\n",
      "3/3 [==============================] - 0s 18ms/step\n",
      "Epoch 20: saved predictions and true labels\n",
      "71/71 - 8s - loss: 1.4822 - accuracy: 0.5788 - top_3_accuracy: 0.8433 - top_5_accuracy: 0.9190 - val_loss: 1.2101 - val_accuracy: 0.7188 - val_top_3_accuracy: 0.9167 - val_top_5_accuracy: 0.9583 - 8s/epoch - 106ms/step\n",
      "Epoch 21/128\n",
      "3/3 [==============================] - 0s 21ms/step\n",
      "Epoch 21: saved predictions and true labels\n",
      "71/71 - 8s - loss: 1.0066 - accuracy: 0.6505 - top_3_accuracy: 0.9027 - top_5_accuracy: 0.9577 - val_loss: 0.8163 - val_accuracy: 0.7396 - val_top_3_accuracy: 0.9375 - val_top_5_accuracy: 0.9896 - 8s/epoch - 106ms/step\n",
      "Epoch 22/128\n",
      "3/3 [==============================] - 0s 18ms/step\n",
      "Epoch 22: saved predictions and true labels\n",
      "71/71 - 8s - loss: 0.9552 - accuracy: 0.6769 - top_3_accuracy: 0.9146 - top_5_accuracy: 0.9608 - val_loss: 1.4948 - val_accuracy: 0.6354 - val_top_3_accuracy: 0.8958 - val_top_5_accuracy: 0.9896 - 8s/epoch - 107ms/step\n",
      "Epoch 23/128\n",
      "3/3 [==============================] - 0s 18ms/step\n",
      "Epoch 23: saved predictions and true labels\n",
      "71/71 - 8s - loss: 0.8763 - accuracy: 0.7091 - top_3_accuracy: 0.9300 - top_5_accuracy: 0.9749 - val_loss: 1.2979 - val_accuracy: 0.7083 - val_top_3_accuracy: 0.8854 - val_top_5_accuracy: 0.9896 - 8s/epoch - 106ms/step\n",
      "Epoch 24/128\n",
      "3/3 [==============================] - 0s 32ms/step\n",
      "Epoch 24: saved predictions and true labels\n",
      "71/71 - 8s - loss: 0.8109 - accuracy: 0.7298 - top_3_accuracy: 0.9318 - top_5_accuracy: 0.9754 - val_loss: 1.4409 - val_accuracy: 0.6562 - val_top_3_accuracy: 0.9375 - val_top_5_accuracy: 1.0000 - 8s/epoch - 107ms/step\n",
      "Epoch 25/128\n",
      "3/3 [==============================] - 0s 20ms/step\n",
      "Epoch 25: saved predictions and true labels\n",
      "71/71 - 8s - loss: 0.7610 - accuracy: 0.7421 - top_3_accuracy: 0.9432 - top_5_accuracy: 0.9789 - val_loss: 1.0208 - val_accuracy: 0.7604 - val_top_3_accuracy: 0.9583 - val_top_5_accuracy: 1.0000 - 8s/epoch - 107ms/step\n",
      "Epoch 26/128\n",
      "3/3 [==============================] - 0s 20ms/step\n",
      "Epoch 26: saved predictions and true labels\n",
      "71/71 - 8s - loss: 0.7297 - accuracy: 0.7557 - top_3_accuracy: 0.9463 - top_5_accuracy: 0.9767 - val_loss: 0.9649 - val_accuracy: 0.7604 - val_top_3_accuracy: 0.9479 - val_top_5_accuracy: 0.9896 - 8s/epoch - 107ms/step\n",
      "Epoch 27/128\n",
      "3/3 [==============================] - 0s 20ms/step\n",
      "Epoch 27: saved predictions and true labels\n",
      "71/71 - 8s - loss: 0.7226 - accuracy: 0.7548 - top_3_accuracy: 0.9467 - top_5_accuracy: 0.9789 - val_loss: 1.3256 - val_accuracy: 0.6875 - val_top_3_accuracy: 0.9583 - val_top_5_accuracy: 1.0000 - 8s/epoch - 106ms/step\n",
      "Epoch 28/128\n",
      "3/3 [==============================] - 0s 26ms/step\n",
      "Epoch 28: saved predictions and true labels\n",
      "71/71 - 8s - loss: 0.7038 - accuracy: 0.7636 - top_3_accuracy: 0.9393 - top_5_accuracy: 0.9793 - val_loss: 1.5664 - val_accuracy: 0.6771 - val_top_3_accuracy: 0.9167 - val_top_5_accuracy: 0.9688 - 8s/epoch - 107ms/step\n",
      "Epoch 29/128\n",
      "3/3 [==============================] - 0s 17ms/step\n",
      "Epoch 29: saved predictions and true labels\n",
      "71/71 - 8s - loss: 0.6560 - accuracy: 0.7830 - top_3_accuracy: 0.9555 - top_5_accuracy: 0.9815 - val_loss: 1.1121 - val_accuracy: 0.7396 - val_top_3_accuracy: 0.9583 - val_top_5_accuracy: 1.0000 - 8s/epoch - 106ms/step\n",
      "Epoch 30/128\n",
      "3/3 [==============================] - 0s 26ms/step\n",
      "Epoch 30: saved predictions and true labels\n",
      "71/71 - 8s - loss: 0.5788 - accuracy: 0.8019 - top_3_accuracy: 0.9613 - top_5_accuracy: 0.9868 - val_loss: 1.2511 - val_accuracy: 0.7500 - val_top_3_accuracy: 0.9583 - val_top_5_accuracy: 0.9896 - 8s/epoch - 106ms/step\n",
      "Epoch 31/128\n",
      "3/3 [==============================] - 0s 18ms/step\n",
      "Epoch 31: saved predictions and true labels\n",
      "71/71 - 8s - loss: 0.5892 - accuracy: 0.8085 - top_3_accuracy: 0.9621 - top_5_accuracy: 0.9899 - val_loss: 0.7373 - val_accuracy: 0.7812 - val_top_3_accuracy: 0.9688 - val_top_5_accuracy: 1.0000 - 8s/epoch - 107ms/step\n",
      "Epoch 32/128\n",
      "3/3 [==============================] - 0s 18ms/step\n",
      "Epoch 32: saved predictions and true labels\n",
      "71/71 - 8s - loss: 0.5736 - accuracy: 0.8072 - top_3_accuracy: 0.9670 - top_5_accuracy: 0.9894 - val_loss: 1.0368 - val_accuracy: 0.8125 - val_top_3_accuracy: 0.9479 - val_top_5_accuracy: 0.9896 - 8s/epoch - 106ms/step\n",
      "Epoch 33/128\n",
      "3/3 [==============================] - 0s 20ms/step\n",
      "Epoch 33: saved predictions and true labels\n",
      "71/71 - 8s - loss: 0.5927 - accuracy: 0.7984 - top_3_accuracy: 0.9648 - top_5_accuracy: 0.9881 - val_loss: 1.2906 - val_accuracy: 0.7396 - val_top_3_accuracy: 0.9583 - val_top_5_accuracy: 1.0000 - 8s/epoch - 106ms/step\n",
      "Epoch 34/128\n",
      "3/3 [==============================] - 0s 18ms/step\n",
      "Epoch 34: saved predictions and true labels\n",
      "71/71 - 7s - loss: 0.5705 - accuracy: 0.8121 - top_3_accuracy: 0.9617 - top_5_accuracy: 0.9850 - val_loss: 0.7872 - val_accuracy: 0.8021 - val_top_3_accuracy: 0.9688 - val_top_5_accuracy: 1.0000 - 7s/epoch - 105ms/step\n",
      "Epoch 35/128\n",
      "3/3 [==============================] - 0s 20ms/step\n",
      "Epoch 35: saved predictions and true labels\n",
      "71/71 - 7s - loss: 0.4992 - accuracy: 0.8327 - top_3_accuracy: 0.9732 - top_5_accuracy: 0.9912 - val_loss: 0.9320 - val_accuracy: 0.7500 - val_top_3_accuracy: 0.9583 - val_top_5_accuracy: 1.0000 - 7s/epoch - 105ms/step\n",
      "Epoch 36/128\n",
      "3/3 [==============================] - 0s 18ms/step\n",
      "Epoch 36: saved predictions and true labels\n",
      "71/71 - 8s - loss: 0.6309 - accuracy: 0.8081 - top_3_accuracy: 0.9591 - top_5_accuracy: 0.9846 - val_loss: 1.2526 - val_accuracy: 0.7812 - val_top_3_accuracy: 0.9271 - val_top_5_accuracy: 0.9688 - 8s/epoch - 106ms/step\n",
      "Epoch 37/128\n",
      "3/3 [==============================] - 0s 18ms/step\n",
      "Epoch 37: saved predictions and true labels\n",
      "71/71 - 8s - loss: 0.4919 - accuracy: 0.8407 - top_3_accuracy: 0.9732 - top_5_accuracy: 0.9943 - val_loss: 1.0414 - val_accuracy: 0.8125 - val_top_3_accuracy: 0.9896 - val_top_5_accuracy: 0.9896 - 8s/epoch - 106ms/step\n",
      "Epoch 38/128\n",
      "3/3 [==============================] - 0s 22ms/step\n",
      "Epoch 38: saved predictions and true labels\n",
      "71/71 - 8s - loss: 0.4484 - accuracy: 0.8451 - top_3_accuracy: 0.9771 - top_5_accuracy: 0.9934 - val_loss: 0.8595 - val_accuracy: 0.7917 - val_top_3_accuracy: 0.9583 - val_top_5_accuracy: 0.9896 - 8s/epoch - 106ms/step\n",
      "Epoch 39/128\n",
      "3/3 [==============================] - 0s 20ms/step\n",
      "Epoch 39: saved predictions and true labels\n",
      "71/71 - 8s - loss: 0.7453 - accuracy: 0.7764 - top_3_accuracy: 0.9503 - top_5_accuracy: 0.9784 - val_loss: 1.1776 - val_accuracy: 0.7500 - val_top_3_accuracy: 0.9583 - val_top_5_accuracy: 0.9896 - 8s/epoch - 107ms/step\n",
      "Epoch 40/128\n",
      "3/3 [==============================] - 0s 19ms/step\n",
      "Epoch 40: saved predictions and true labels\n",
      "71/71 - 8s - loss: 0.8224 - accuracy: 0.7579 - top_3_accuracy: 0.9313 - top_5_accuracy: 0.9723 - val_loss: 1.4263 - val_accuracy: 0.7188 - val_top_3_accuracy: 0.9062 - val_top_5_accuracy: 0.9583 - 8s/epoch - 106ms/step\n",
      "Epoch 41/128\n",
      "3/3 [==============================] - 0s 26ms/step\n",
      "Epoch 41: saved predictions and true labels\n",
      "71/71 - 7s - loss: 0.5381 - accuracy: 0.8116 - top_3_accuracy: 0.9683 - top_5_accuracy: 0.9903 - val_loss: 1.1528 - val_accuracy: 0.7812 - val_top_3_accuracy: 0.9479 - val_top_5_accuracy: 1.0000 - 7s/epoch - 106ms/step\n",
      "Epoch 42/128\n",
      "3/3 [==============================] - 0s 21ms/step\n",
      "Epoch 42: saved predictions and true labels\n",
      "71/71 - 8s - loss: 0.5125 - accuracy: 0.8239 - top_3_accuracy: 0.9736 - top_5_accuracy: 0.9903 - val_loss: 1.8872 - val_accuracy: 0.6771 - val_top_3_accuracy: 0.9375 - val_top_5_accuracy: 0.9583 - 8s/epoch - 106ms/step\n",
      "Epoch 43/128\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class SavePredictionsCallback(Callback):\n",
    "    def __init__(self, X_test, y_test):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.predictions_per_epoch = []\n",
    "        self.true_labels_per_epoch = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        self.predictions_per_epoch.append(y_pred)\n",
    "        self.true_labels_per_epoch.append(self.y_test)\n",
    "        print(f\"Epoch {epoch+1}: saved predictions and true labels\")\n",
    "\n",
    "if config_params['training']['optimizer'] == 'adam':\n",
    "    optimizer = optimizers.Adam(learning_rate=0.01, epsilon=0.001)\n",
    "    \n",
    "elif config_params['training']['optimizer'] == 'sgd':\n",
    "    optimizer = optimizers.SGD(learning_rate=learning_late, momentum=0.9)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', top_3_accuracy, top_5_accuracy])\n",
    "\n",
    "train_callbacks = []\n",
    "\n",
    "lrScheduler = MyLRScheduler(**config_params['training']['l_rate_schedule'])\n",
    "train_callbacks.append(lrScheduler)\n",
    "#print(train_generator)\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "save_predictions_callback = SavePredictionsCallback(X_test, Y_test)\n",
    "\n",
    "history = model.fit(train_generator, epochs=128, \n",
    "                              validation_data=(X_test,Y_test), callbacks=[save_predictions_callback], verbose=2)\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "y_test = np.argmax(Y_test, axis=1)\n",
    "    \n",
    "#print(history.history)\n",
    "\n",
    "#acc_list.append(max(history.history['accuracy']))\n",
    "\n",
    "if config_params['model']['save']:\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(MODEL_SAVE_FILE.format(SUBJECT), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(MODEL_WEIGHTS_SAVE_FILE.format(SUBJECT))\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "# C_{i, j} is equal to the number of observations known to be in group i but predicted to be in group j.\n",
    "cnf_matrix_frame = metrics.confusion_matrix(y_test, y_pred)\n",
    "if np.array(mean_cm).shape != cnf_matrix_frame.shape:\n",
    "    mean_cm = cnf_matrix_frame\n",
    "else:\n",
    "    mean_cm += cnf_matrix_frame\n",
    "\n",
    "#dict_keys(['loss', 'accuracy', 'top_3_accuracy', 'top_5_accuracy', 'val_loss', 'val_accuracy', 'val_top_3_accuracy', 'val_top_5_accuracy'])\n",
    "\n",
    "mean_train.append(history.history['accuracy'][-1])\n",
    "mean_test.append(history.history['val_accuracy'][-1])\n",
    "mean_train_loss.append(history.history['loss'][-1])\n",
    "mean_test_loss.append(history.history['val_loss'][-1])\n",
    "mean_test_3.append(history.history['val_top_3_accuracy'][-1])\n",
    "mean_test_5.append(history.history['val_top_5_accuracy'][-1])\n",
    "\n",
    "if config_params['logging']['enable']:\n",
    "    with open(LOGGING_FILE, 'a') as f:\n",
    "        f.write('{},{},{},{},{},{},{},{},{}\\n'.format(SUBJECT, train_generator.__len__() * PARAMS_TRAIN_GENERATOR['batch_size'], valid_generator.__len__(),\n",
    "            mean_train_loss[-1], mean_train[-1], mean_test_loss[-1], mean_test[-1], mean_test_3[-1], mean_test_5[-1]))\n",
    "\n",
    "metrics_dict = {\n",
    "    'mean_cm': mean_cm,\n",
    "    'mean_test': mean_test,\n",
    "    'mean_test_3': mean_test_3,\n",
    "    'mean_test_5': mean_test_5,\n",
    "    'mean_train': mean_train,\n",
    "    'mean_train_loss': mean_train_loss,\n",
    "    'mean_test_loss': mean_test_loss\n",
    "}\n",
    "scipy.io.savemat(METRICS_SAVE_FILE.format(SUBJECT), metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da6889-6845-40d1-829c-6e0aa6cf08a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"-----subject-------\")\n",
    "print(target_subject)\n",
    "print(\"----accuracy-----\")\n",
    "print(max(history.history['accuracy']))\n",
    "print(\"---val accuracy---\")\n",
    "print(max(history.history['val_accuracy']))\n",
    "\n",
    "predictions_per_epoch = save_predictions_callback.predictions_per_epoch\n",
    "true_labels_per_epoch = save_predictions_callback.true_labels_per_epoch\n",
    "\n",
    "for epoch, (y_pred, y_true) in enumerate(zip(predictions_per_epoch, true_labels_per_epoch)):\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    if (round(metrics.accuracy_score(y_test, y_pred),4) == round(max(history.history['val_accuracy']),4)):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        # print(\"Predictions:\", y_pred)\n",
    "        # print(\"True Labels:\", y_true)\n",
    "\n",
    "        report = classification_report(y_test, y_pred, digits=4)\n",
    "        print(report)\n",
    "\n",
    "target_subject += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff8a874-6929-48ff-9005-fdeecbf8e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/home/sota/EMGcode/temp_result'\n",
    "file_name = f'DB5_report{target_subject-1}.txt'\n",
    "file_path = os.path.join(directory, file_name)\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(report)\n",
    "\n",
    "print(f\"Classification report saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac30a765-43af-47cc-b1e3-9792769f1141",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PARAMS_TRAIN_GENERATOR)\n",
    "print('---------------------------------------------------------------------------\\n')\n",
    "print(PARAMS_VALID_GENERATOR)\n",
    "print('---------------------------------------------------------------------------\\n')\n",
    "print(train_generator)\n",
    "print('---------------------------------------------------------------------------\\n')\n",
    "print(valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a858e88d-5fbe-4451-af17-c6d0b4a20451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "636"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.io\n",
    "\n",
    "Dictionary = scipy.io.loadmat('/home/sota/EMGcode/EMG_experiment/DB5_pre/subject-01/gesture-04/rms/rep-02.mat')\n",
    "len(Dictionary['emg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660182ab-136e-4288-8929-710dc8258da0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
